We can read more about k6 [here](https://k6.io/docs/getting-started/running-k6).

To run a test xxxxx.js:

```ps
k6 run -vus 10 --duration 30s xxxxx.js
```

# Life Cycle 

Read more about the different [life cycle stages of a k6 test](https://k6.io/docs/using-k6/test-life-cycle).

There are four distinct life cycle stages to a k6 test that can be controlled by you, the user. They are the "init", "setup", "vu" and "teardown" stages. 

```js
// 1. init code

export function setup() {
  // 2. setup code
}

export default function(data) {
  // 3. vu code
}

export function teardown(data) {
  // 4. teardown code
}
```

Code inside default is called "VU code", and is run over and over for as long as the test is running. Code outside of it is called "init code", and is run only once per VU.

VU code can make HTTP requests, emit metrics, and generally do everything you'd expect a load test to do - with a few important exceptions: __you can't load anything from your local filesystem, or import any other modules__. This all __has to be done from the init code__.

There are two reasons for this. The first is, of course: performance.

If you read a file from disk on every single script iteration, it'd be needlessly slow; even if you cache the contents of the file and any imported modules, it'd mean the first run of the script would be much slower than all the others. Worse yet, if you have a script that imports or loads things based on things that can only be known at runtime, you'd get slow iterations thrown in every time you load something new.

But there's another, more interesting reason. By forcing all imports and file reads into the init context, we make an important design goal possible; we want to support three different execution modes without the need for you to modify your scripts; local, cloud and clustered execution. In the case of cloud and clustered execution we know which files will be needed, so we distribute only those files. We know which modules will be imported, so we can bundle them up from the get-go. And, tying into the performance point above, the other nodes don't even need writable filesystems - everything can be kept in-memory.

As an added bonus, __you can use this to reuse data between iterations (but only for the same VU)__:

```js
var counter = 0;

export default function() {
  counter++;
}
```

## The default function life-cycle

A VU will execute the default function from start to end in sequence. Nothing out of the ordinary so far, but here's the important part; once the VU reaches the end of the default function it will loop back to the start and execute the code all over.

__As part of this "restart" process, the VU is reset__. __Cookies are cleared and TCP connections might be torn down__, depending on your test configuration options. __Make sure to use sleep() statements to pace your VUs properly__. __An appropriate amount of sleep/think time at the end of the default function is often needed to properly simulate a user reading content on a page__. If you don't have a sleep() statement at the end of the default function your VU might be more "aggressive" than you've planned. __VU without any sleep() is akin to a user who constantly presses F5 to refresh the page__.

## Setup and teardown stages

Beyond the required init and VU stages, which is code run for each VU, k6 also supports __test-wide setup and teardown stages__, like many other testing frameworks and tools. The setup and teardown functions, like the default function, __needs to be exported functions__. But unlike the default function setup and teardown __are only called once for a test__. __setup is called at the beginning of the test__, __after the init stage__ but __before the VU stage__ (default function), and __teardown__ is called at the __end of a test__, __after the VU stage__ (default function). Therefore, VU number is 0 while executing the setup and teardown functions.

Again, let's have a look at the basic structure of a k6 test:

```js
// 1. init code

export function setup() {
  // 2. setup code
}

export default function(data) {
  // 3. vu code
}

export function teardown(data) {
  // 4. teardown code
}
```

Notice the function signature of the default function and teardown function takes an argument, which we here refer to as data.

This __data will be whatever is returned in the setup function__, so a mechanism for passing data from the setup stage to the subsequent VU and teardown stages in a way that, again, is compatible with our goal of supporting local, cloud and clustered execution modes without requiring script changes when switching between them. (it might or might not be the same node that runs the setup and teardown stages in the cloud or clustered execution mode).

To support all of those modes, only data (i.e. JSON) can be passed between setup() and the other stages, any passed functions will be stripped.

Here's an example of doing just that, passing some data from setup to VU and teardown stages:

```js
export function setup() {
  return { v: 1 };
}

export default function(data) {
  console.log(JSON.stringify(data));
}

export function teardown(data) {
  if (data.v != 1) {
    throw new Error("incorrect data: " + JSON.stringify(data));
  }
}
```

A big difference between the init stage and setup/teardown stages is that you have the full k6 API available in the latter, you can for example make HTTP requests in the setup and teardown stages:

```js
export function setup() {
  let res = http.get("https://httpbin.org/get");
  return { data: res.json() };
}

export function teardown(data) {
  console.log(JSON.stringify(data));
}

export default function(data) {
  console.log(JSON.stringify(data));
}
```

Note that any requests made in the setup and teardown stages will be counted in the end-of-test summary. Those requests will be tagged appropriately with the ::setup and ::teardown values for the group metric tag, so that you can filter them in JSON output or InfluxDB.


# Options

## VUs & Duration

If you want to avoid having to type --vus 10 and --duration 30s all the time, you can include those settings inside your JavaScript file also:

```js
export let options = {
  vus: 10,
  duration: '30s',
};
```

## Stages: ramping up/down VUs

You can also have the VU level ramp up and down during the test. The options.stages property allows you to configure ramping behaviour.

For example, with this configuration we set a total duration of 10s. At 3s we will have 2VU, at 8s 5VU, and finally, at 10s 0VU:

```js
export let options = {
  stages: [
    { duration: '3s', target: 2 },
    { duration: '5s', target: 5 },
    { duration: '2s', target: 0 },
  ],
};
```

## Thresholds y Metricas

We can specify thresholds for the test, that will be evaluated while the test is running, and shown as a summary at the end of the execution, together with the default metrics. We can also define that the test is to be aborted in case the threshold is not met.

The thresholds specify the metric and the kpi that has to be met. There are a number of estandard [metrics](https://k6.io/docs/using-k6/metrics#http-specific-built-in-metrics) such as:
- http_req_failed. Peticiones http fallidas
- http_req_duration. Duracion de peticiones http

We can also define custom metrics. The custom metrics must belong to one of the basic types:

```js
import { Trend, Counter, Gauge, Rate  } from 'k6/metrics';
```

- Trend. Allows to calculate the average, max, min and percentile of a series
- Counter. Accumaltes the values of a series
- Gauge. Stores the last value of a series
- Rate. Calculates the percentage of non zero - false being considered as zero - values

### Metrics

We define the metrics as part of the initialization phase:

```js
//Permite calcular la media, mínimo, máximo y percentiles de una serie de valores
export let TrendRTT = new Trend('RTT');
//Determina el porcentaje de valores que no son cero
export let RateContentOK = new Rate('Contenido OK');
//Almacena el último valor en una serie de valores
export let GaugeContentSize = new Gauge('Tamaño');
//Acumula en la métrica los valores de la serie
export let CounterErrors = new Counter('Errors');
```

The metrics will be given a value during the execution of the test:

```js
  let res = http.get('https://test-api.k6.io/public/crocodiles/1/');
  let contentOK = res.json('name') === 'Bert';
  //Trend
  TrendRTT.add(res.timings.duration);
  //Rate
  RateContentOK.add(contentOK);
  //Gauge
  GaugeContentSize.add(res.body.length);
  //Counter
  CounterErrors.add(!contentOK);
```

Note that we will have one instance of each per VU.

### Summary of execution

When the test is completed, [a number of metrics/KPIs will be shown](https://k6.io/docs/using-k6/metrics#built-in-metrics).

### Thresholds

On top of the default metrics/KPIs shown by default, we may define thresholds. The threshold will define for a given metric what is the acceptable value that one or several KPIs may take. During the tests these values will be calculated, and shown at the end of the test. If shown in "green" it will mean that the threshold was not crossed, otherwise they will be shown in "red".

The thresholds are defined in the options. This is an example:

```js
export let options = {
thresholds: {
  http_req_failed: ['rate<0.01'],   
  http_req_duration: ['p(90) < 150', 'p(95) < 200', 'p(99.9) < 2000'],
  'http_req_duration{type: miAPI }': ['p(90) < 150', 'p(95) < 200', 'p(99.9) < 2000'],
  'duracionGrupo{nombre:peticionesIndividuales}': [{ threshold: 'avg < 400', abortOnFail: true }],
  'duracionGrupo{nombre:peticionesBatch}': ['avg < 150'],
  RTT: ['p(99)<300', 'p(70)<250', 'avg<200', 'med<150', 'min<100'],
  'Contenido OK': ['rate>0.95'],
  'Tamaño': ['value<4000'],
  Errors: ['count<100'],
},
```

We can use standard metrics such as `http_req_failed` and `http_req_duration`, or custom ones such as `Errors` or `RTT`. We then define a threshold per metric:

```js
http_req_failed: ['rate<0.01'],
```

or several:

```js
http_req_duration: ['p(90) < 150', 'p(95) < 200', 'p(99.9) < 2000'],
```

The threshold expresion we use depends on the type of metric.
- Rate. `'rate<0.01'` the rate is under 1%
- Trend. 
    - `'p(90) < 150'`. Percentile 90 is under 150ms
    - `'avg<200'`. Average is under 200ms
    - `'med<150'`. Median is under 150
    - `'min<100'`. Minimum is under 100
    - etc.
- Counter. `'count<100'`. Count is under 100
- Gauge. `'value<4000'`. Value is under 4000

In the definition of the threshold we can specify whether we want to stop the test in case a threshold is crossed or not. By default is not stopped, but if we do want to stop it we would specify `abortOnFail: true`:

```js
 'duracionGrupo{nombre:peticionesIndividuales}': [{ threshold: 'avg < 400', abortOnFail: true }]
```

#### Tags

Finally, we can add tags to the metrics, so that the kpis are calculated for metrics of that particular tag. Here for example we define a threshold for `http_req_duration` of those http requests with a tag `{type: miAPI }`:

```js
  'http_req_duration{type: miAPI }': ['p(90) < 150', 'p(95) < 200', 'p(99.9) < 2000'],
```

When making an http request we can specify the tag:

```js
  let r = http.get(`http://test.k6.io`, {
    tags: { type: 'miAPI' },
  });
```

We can also define tags in custom metrics. Suppose we have this metric:

```js
let groupDuration = Trend('duracionGrupo');
```

We may set a tag with the metric:

```js
groupDuration.add(end - start, { nombre: `peticionesBatch' });
```

So now the threshold could be:

```js
 export let options = {
  thresholds: {
    //METRICAS CUSTOM
    'duracionGrupo{nombre:peticionesBatch}': ['avg < 150'],
  },
};
```

# Init

The init is performed once per VU. Here we can specify custom metrics

```js
//Permite calcular la media, mínimo, máximo y percentiles de una serie de valores
let TrendRTT = new Trend('RTT');
//Determina el porcentaje de valores que no son cero
let RateContentOK = new Rate('Contenido OK');
//Almacena el último valor en una serie de valores
let GaugeContentSize = new Gauge('Tamaño');
//Acumula en la métrica los valores de la serie
let CounterErrors = new Counter('Errors');
```
any object that will be part of the context of each VU, or any cross cutting concern:

```js
let cuenta= 0;
```

We can use these values as part of the test, but is important to note that each VU will have its own copy of these values. For instance here we are incrementing the variable `cuenta`, but we have as many variables `cuenta` as VUs we have. We will see in the execution log different printouts of the variable for different VUs:

```js
export default function (data) {
  cuenta=cuenta+1;
  console.log(`Numero de ciclos: ${cuenta}`);
```

# Setup

It is run once per test, regardles of the number of VUs. It is ran once the initialization for each VU has taken place. Here we can do anything related to preparing the test, from loading a file, making an http or gRPC request, or just initializing any variable.

The setup is implemented in the method `export function setup() {...}`. This method may retunr an object. This object will be fed to the test and teardown - more later about teardown. We can also apply asserts - more later. For example: 

```js
export function setup() {
  let res = http.get("https://httpbin.org/get");

  //Aserta el contenido de la respuesta
  //Al terminar el test veremos el resultado de estas verificaciones dentro de la sección setup
  check(res, {
    'Ok': (val) => res.status === 200,
    'Hay payload': (val) => res.body.length > 0,
  });

  //Este objeto estará disponible en el test
  return { datos: res.json(),v:1 };
}
```

# Teardown

Once the test is completed, the `export function teardown(data) {}` will be executed. The data that is passed as argument is the same object that was returned by the `setup()` method:

```js
//Los datos de entrada son los que produce la función export function setup()
export function teardown(data) {
  if (data.v != 1) {
    throw new Error("incorrect data: " + JSON.stringify(data));
  }
  console.log(JSON.stringify(data.datos));
}
```

# Test

The test is implemented in `export default function (data) {}`. This method will be executed per multiple times per VU. The data that is passed as argument is the object returned by the `setup()` method.

## http calls

We can make gRPC, websocket and http requests. Lets see the case for http:

```js
import http from "k6/http";
```

We can make individual or batch calls:

```js
let r=http.get('https://test-api.k6.io/public/crocodiles/1/');
```

```js
let r=http.batch([
    ['GET', `https://test-api.k6.io/public/crocodiles/1/`],
    ['GET', `https://test-api.k6.io/public/crocodiles/2/`],
    ['GET', `https://test-api.k6.io/public/crocodiles/3/`],
  ]);
```

The response contains a [number of fields](https://k6.io/docs/using-k6/metrics#accessing-http-timings-from-a-script) we can use. For example here we are looking into the _http status code_ and the _body lenght_ of the response:

```js
  check(r, {
    'Ok': (val) => val.status === 200,
    'Tamaño del body mayor que 1176 bytes': (val) => val.body.length > 1176,
  });
```

Finally, we can tag the calls, so that later one we may define specific thresholds per request & tag. Here we are specifying a tag:

```js
  let r = http.get(`http://test.k6.io`, {
    tags: { type: 'miAPI' },
  });
```

This will enable the creation of a threshold like this:

```js
export let options = {
thresholds: {
  'http_req_duration{type: miAPI }': ['p(90) < 150', 'p(95) < 200', 'p(99.9) < 2000'],
```

## Assertions

To use assertions we import:

```js
import { check } from 'k6';
```

The assertions can be included in the _setup_, _default_ and _teardown_ methods. The result of the assertion is displayed in the summary of the execution. The assertions that are met will be displayed in green, whereas the ones not met in red.

```js
  check(data, {
    'Valor OK': val => val.v === 1,
    'httpbin OK': val => "https://httpbin.org/get" == val.datos.url
  });

  //Aserta el contenido de la respuesta
  check(r, {
    'Ok': (val) => val.status === 200,
    'Tamaño del body mayor que 1176 bytes': (val) => val.body.length > 1176,
  });
```

## Pauses

In the design of the test we have to introduce pauses that emulate the actual behaviour of the user or the process. To introduce such pauses we use `sleep`:

```js
import { sleep } from 'k6';
```

```js
  sleep(3);
```

## Groupe Actions together

When we want to deal with some actions as a single "block", we will groupe them. To group actions we use `group`:

```js
import { group } from 'k6';
```

We can group together metrics using the `group` function. All the metrics/kpis can be disected by group:

```js
group(name, group_function);
```

```js
  group('peticionesIndividuales', function () {
    http.get('https://test-api.k6.io/public/crocodiles/1/');
    http.get('https://test-api.k6.io/public/crocodiles/2/');
    http.get('https://test-api.k6.io/public/crocodiles/3/');
  });
```